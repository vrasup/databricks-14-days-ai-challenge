{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1620f65c-8c34-46c0-a156-affff0891eba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /local_disk0/.ephemeral_nfs/envs/pythonEnv-60ec64ea-cf68-4862-8229-94802b5a8a5b/lib/python3.12/site-packages (1.8.3)\nRequirement already satisfied: black>=24.10.0 in /databricks/python3/lib/python3.12/site-packages (from kaggle) (24.10.0)\nRequirement already satisfied: bleach in /databricks/python3/lib/python3.12/site-packages (from kaggle) (6.2.0)\nRequirement already satisfied: kagglesdk<1.0,>=0.1.14 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-60ec64ea-cf68-4862-8229-94802b5a8a5b/lib/python3.12/site-packages (from kaggle) (0.1.14)\nRequirement already satisfied: mypy>=1.15.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-60ec64ea-cf68-4862-8229-94802b5a8a5b/lib/python3.12/site-packages (from kaggle) (1.19.1)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.12/site-packages (from kaggle) (24.1)\nRequirement already satisfied: protobuf in /databricks/python3/lib/python3.12/site-packages (from kaggle) (5.29.4)\nRequirement already satisfied: python-dateutil in /databricks/python3/lib/python3.12/site-packages (from kaggle) (2.9.0.post0)\nRequirement already satisfied: python-slugify in /local_disk0/.ephemeral_nfs/envs/pythonEnv-60ec64ea-cf68-4862-8229-94802b5a8a5b/lib/python3.12/site-packages (from kaggle) (8.0.4)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.12/site-packages (from kaggle) (2.32.3)\nRequirement already satisfied: six>=1.10 in /usr/lib/python3/dist-packages (from kaggle) (1.16.0)\nRequirement already satisfied: tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-60ec64ea-cf68-4862-8229-94802b5a8a5b/lib/python3.12/site-packages (from kaggle) (4.67.1)\nRequirement already satisfied: types-requests in /local_disk0/.ephemeral_nfs/envs/pythonEnv-60ec64ea-cf68-4862-8229-94802b5a8a5b/lib/python3.12/site-packages (from kaggle) (2.32.4.20260107)\nRequirement already satisfied: types-tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-60ec64ea-cf68-4862-8229-94802b5a8a5b/lib/python3.12/site-packages (from kaggle) (4.67.0.20250809)\nRequirement already satisfied: urllib3>=1.15.1 in /databricks/python3/lib/python3.12/site-packages (from kaggle) (2.3.0)\nRequirement already satisfied: click>=8.0.0 in /databricks/python3/lib/python3.12/site-packages (from black>=24.10.0->kaggle) (8.1.7)\nRequirement already satisfied: mypy-extensions>=0.4.3 in /databricks/python3/lib/python3.12/site-packages (from black>=24.10.0->kaggle) (1.0.0)\nRequirement already satisfied: pathspec>=0.9.0 in /databricks/python3/lib/python3.12/site-packages (from black>=24.10.0->kaggle) (0.10.3)\nRequirement already satisfied: platformdirs>=2 in /databricks/python3/lib/python3.12/site-packages (from black>=24.10.0->kaggle) (3.10.0)\nRequirement already satisfied: typing_extensions>=4.6.0 in /databricks/python3/lib/python3.12/site-packages (from mypy>=1.15.0->kaggle) (4.12.2)\nRequirement already satisfied: librt>=0.6.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-60ec64ea-cf68-4862-8229-94802b5a8a5b/lib/python3.12/site-packages (from mypy>=1.15.0->kaggle) (0.7.7)\nRequirement already satisfied: webencodings in /databricks/python3/lib/python3.12/site-packages (from bleach->kaggle) (0.5.1)\nRequirement already satisfied: text-unidecode>=1.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-60ec64ea-cf68-4862-8229-94802b5a8a5b/lib/python3.12/site-packages (from python-slugify->kaggle) (1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests->kaggle) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests->kaggle) (3.7)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests->kaggle) (2025.1.31)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "780fb193-d7dd-4638-9733-615b5bd59550",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle credentials configured!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Replace with your Kaggle credentials from kaggle.json\n",
    "os.environ['KAGGLE_USERNAME'] = \"vidhyarasu\"\n",
    "os.environ['KAGGLE_KEY'] = \"c3bbd36a1d5fb7bfd5f45e28614043a5\"\n",
    "\n",
    "# Verify setup\n",
    "print(\"Kaggle credentials configured!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1441e9c-d2bb-4706-bbb8-9ec65acceab1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already exist, skipping download.\ntotal 14G\n-rwxrwxrwx 1 nobody nogroup 8.4G Jan  7 23:04 2019-Nov.csv\n-rwxrwxrwx 1 nobody nogroup 5.3G Jan  7 23:06 2019-Oct.csv\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "cd /Volumes/workspace/ecommerce/ecommerce_data\n",
    "\n",
    "# Only download if Oct file doesn't exist\n",
    "if [ ! -f \"2019-Oct.csv\" ]; then\n",
    "    echo \"Downloading dataset...\"\n",
    "    kaggle datasets download -d mkechinov/ecommerce-behavior-data-from-multi-category-store\n",
    "    unzip -o ecommerce-behavior-data-from-multi-category-store.zip\n",
    "    rm -f ecommerce-behavior-data-from-multi-category-store.zip\n",
    "else\n",
    "    echo \"Files already exist, skipping download.\"\n",
    "fi\n",
    "\n",
    "ls -lh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac55af75-f33a-4de7-b2e8-1b8f4f39235b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "October events: 42,448,764\nNovember events: 67,501,979\nTotal combined events: 109,950,743\n+-------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\n|         event_time|event_type|product_id|        category_id|       category_code|   brand|  price|  user_id|        user_session|\n+-------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\n|2019-10-01 00:00:00|      view|  44600062|2103807459595387724|                NULL|shiseido|  35.79|541312140|72d76fde-8bb3-4e0...|\n|2019-10-01 00:00:00|      view|   3900821|2053013552326770905|appliances.enviro...|    aqua|   33.2|554748717|9333dfbd-b87a-470...|\n|2019-10-01 00:00:01|      view|  17200506|2053013559792632471|furniture.living_...|    NULL|  543.1|519107250|566511c2-e2e3-422...|\n|2019-10-01 00:00:01|      view|   1307067|2053013558920217191|  computers.notebook|  lenovo| 251.74|550050854|7c90fc70-0e80-459...|\n|2019-10-01 00:00:04|      view|   1004237|2053013555631882655|electronics.smart...|   apple|1081.98|535871217|c6bd7419-2748-4c5...|\n+-------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Load October data\n",
    "oct_events = spark.read.csv(\n",
    "    \"/Volumes/workspace/ecommerce/ecommerce_data/2019-Oct.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Optional: Load November data\n",
    "nov_events = spark.read.csv(\n",
    "    \"/Volumes/workspace/ecommerce/ecommerce_data/2019-Nov.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Combine both months if needed\n",
    "all_events = oct_events.union(nov_events)\n",
    "\n",
    "# Quick verification\n",
    "print(f\"October events: {oct_events.count():,}\")\n",
    "print(f\"November events: {nov_events.count():,}\")\n",
    "print(f\"Total combined events: {all_events.count():,}\")\n",
    "\n",
    "oct_events.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92a9126b-d716-4c88-94d3-8e248dd06200",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique events in Oct: 42418544\nUnique product IDs: 166794\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Count unique rows\n",
    "print(\"Unique events in Oct:\", oct_events.dropDuplicates().count())\n",
    "\n",
    "# Count unique product IDs (optional)\n",
    "print(\"Unique product IDs:\", oct_events.select(\"product_id\").distinct().count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c54f7633-89f9-4492-99b1-e15aee4f9405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Deleted all Spark DataFrames from memory\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Delete all Spark DataFrames from memory\n",
    "for v in list(globals().keys()):\n",
    "    if isinstance(globals()[v], DataFrame):\n",
    "        del globals()[v]\n",
    "\n",
    "print(\"✅ Deleted all Spark DataFrames from memory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ccbb212-1bfe-471a-b136-771d8f271666",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Define paths\n",
    "OCT_PATH = \"/Volumes/workspace/ecommerce/ecommerce_data/2019-Oct.csv\"\n",
    "NOV_PATH = \"/Volumes/workspace/ecommerce/ecommerce_data/2019-Nov.csv\"\n",
    "\n",
    "def load_ecommerce_data(month=\"Oct\", sample_fraction=None):\n",
    "    \"\"\"\n",
    "    Load e-commerce data for specified month from workspace paths.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    month : str\n",
    "        \"Oct\" or \"Nov\" or \"both\"\n",
    "    sample_fraction : float, optional\n",
    "        Fraction of data to sample (0.0 to 1.0) for testing.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Spark DataFrame with events\n",
    "    \"\"\"\n",
    "\n",
    "    if month == \"Oct\":\n",
    "        df = spark.read.csv(OCT_PATH, header=True, inferSchema=True)\n",
    "    elif month == \"Nov\":\n",
    "        df = spark.read.csv(NOV_PATH, header=True, inferSchema=True)\n",
    "    elif month == \"both\":\n",
    "        oct_df = spark.read.csv(OCT_PATH, header=True, inferSchema=True)\n",
    "        nov_df = spark.read.csv(NOV_PATH, header=True, inferSchema=True)\n",
    "        df = oct_df.union(nov_df)\n",
    "    else:\n",
    "        raise ValueError(\"Month must be 'Oct', 'Nov', or 'both'\")\n",
    "\n",
    "    # Apply sampling if requested\n",
    "    if sample_fraction is not None:\n",
    "        if not 0.0 < sample_fraction <= 1.0:\n",
    "            raise ValueError(\"sample_fraction must be between 0.0 and 1.0\")\n",
    "        df = df.sample(fraction=sample_fraction, seed=42)\n",
    "        print(f\"✅ Sampled {sample_fraction*100:.1f}% of data\")\n",
    "\n",
    "    print(f\"✅ Loaded {df.count():,} events for {month}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5139e810-a2e1-4df3-ab25-3991994bd401",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 42,448,764 events for Oct\n✅ Ready to go! Loaded 42,448,764 events\n+-------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\n|         event_time|event_type|product_id|        category_id|       category_code|   brand|  price|  user_id|        user_session|\n+-------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\n|2019-10-01 00:00:00|      view|  44600062|2103807459595387724|                NULL|shiseido|  35.79|541312140|72d76fde-8bb3-4e0...|\n|2019-10-01 00:00:00|      view|   3900821|2053013552326770905|appliances.enviro...|    aqua|   33.2|554748717|9333dfbd-b87a-470...|\n|2019-10-01 00:00:01|      view|  17200506|2053013559792632471|furniture.living_...|    NULL|  543.1|519107250|566511c2-e2e3-422...|\n|2019-10-01 00:00:01|      view|   1307067|2053013558920217191|  computers.notebook|  lenovo| 251.74|550050854|7c90fc70-0e80-459...|\n|2019-10-01 00:00:04|      view|   1004237|2053013555631882655|electronics.smart...|   apple|1081.98|535871217|c6bd7419-2748-4c5...|\n+-------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Load your data\n",
    "events = load_ecommerce_data(\"Oct\")\n",
    "\n",
    "# Verify it's working\n",
    "print(f\"✅ Ready to go! Loaded {events.count():,} events\")\n",
    "events.show(5)\n",
    "\n",
    "# Your Day 1 challenges start here...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff426763-0eed-42de-b9cb-4391ed466021",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All Spark DataFrame variables deleted from memory.\n"
     ]
    }
   ],
   "source": [
    "# Import Spark SQL module\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Delete all Spark DataFrame variables from Python memory\n",
    "for name in list(globals().keys()):\n",
    "    if isinstance(globals()[name], DataFrame):\n",
    "        del globals()[name]\n",
    "\n",
    "print(\"✅ All Spark DataFrame variables deleted from memory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e091f863-3f3c-4a12-b29e-00953e064cab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# ONE-CELL DATA LOADER FOR E-COMMERCE\n",
    "# -----------------------------\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session if not already\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "def load_ecommerce_data(month=\"Oct\", sample_fraction=None):\n",
    "    \"\"\"\n",
    "    Load e-commerce dataset for serverless Databricks.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    month : str\n",
    "        \"Oct\", \"Nov\", or \"both\"\n",
    "    sample_fraction : float, optional\n",
    "        Fraction of data to sample (0 < fraction <= 1)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Spark DataFrame\n",
    "    \"\"\"\n",
    "    path_base = \"/Volumes/workspace/ecommerce/ecommerce_data/\"\n",
    "\n",
    "    if month == \"Oct\":\n",
    "        df = spark.read.csv(f\"{path_base}2019-Oct.csv\", header=True, inferSchema=True)\n",
    "    elif month == \"Nov\":\n",
    "        df = spark.read.csv(f\"{path_base}2019-Nov.csv\", header=True, inferSchema=True)\n",
    "    elif month == \"both\":\n",
    "        oct_df = spark.read.csv(f\"{path_base}2019-Oct.csv\", header=True, inferSchema=True)\n",
    "        nov_df = spark.read.csv(f\"{path_base}2019-Nov.csv\", header=True, inferSchema=True)\n",
    "        df = oct_df.union(nov_df)\n",
    "    else:\n",
    "        raise ValueError(\"Month must be 'Oct', 'Nov', or 'both'\")\n",
    "\n",
    "    if sample_fraction is not None:\n",
    "        df = df.sample(fraction=sample_fraction, seed=42)\n",
    "        print(f\"✅ Sampled {sample_fraction*100}% of data\")\n",
    "\n",
    "    print(f\"✅ Loaded {df.count():,} events for {month}\")\n",
    "    return df\n",
    "\n",
    "# -----------------------------\n",
    "# Load your DataFrame here\n",
    "# -----------------------------\n",
    "events = load_ecommerce_data(\"Oct\")  # change month/sample_fraction as needed\n",
    "\n",
    "# Quick verification\n",
    "events.show(5)\n",
    "events.printSchema()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6238999852438606,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Day_01 Platform setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}