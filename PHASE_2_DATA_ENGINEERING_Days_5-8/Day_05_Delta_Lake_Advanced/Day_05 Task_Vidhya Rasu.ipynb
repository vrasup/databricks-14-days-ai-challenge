{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "208bb95a-57d0-4bfc-a7c5-0137d09da5e4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "outputs": [],
   "source": [
    "#Step 0: Import required modules\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a42466f1-bf8e-4994-ae0e-2084477b3c9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\n|         event_time|event_type|product_id|        category_id|       category_code|   brand|  price|  user_id|        user_session|\n+-------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\n|2019-10-01 00:00:00|      view|  44600062|2103807459595387724|                NULL|shiseido|  35.79|541312140|72d76fde-8bb3-4e0...|\n|2019-10-01 00:00:00|      view|   3900821|2053013552326770905|appliances.enviro...|    aqua|   33.2|554748717|9333dfbd-b87a-470...|\n|2019-10-01 00:00:01|      view|  17200506|2053013559792632471|furniture.living_...|    NULL|  543.1|519107250|566511c2-e2e3-422...|\n|2019-10-01 00:00:01|      view|   1307067|2053013558920217191|  computers.notebook|  lenovo| 251.74|550050854|7c90fc70-0e80-459...|\n|2019-10-01 00:00:04|      view|   1004237|2053013555631882655|electronics.smart...|   apple|1081.98|535871217|c6bd7419-2748-4c5...|\n+-------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\nonly showing top 5 rows\nroot\n |-- event_time: timestamp (nullable = true)\n |-- event_type: string (nullable = true)\n |-- product_id: integer (nullable = true)\n |-- category_id: long (nullable = true)\n |-- category_code: string (nullable = true)\n |-- brand: string (nullable = true)\n |-- price: double (nullable = true)\n |-- user_id: integer (nullable = true)\n |-- user_session: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Read your CSV into a DataFrame\n",
    "\n",
    "#Path to your CSV on the volume\n",
    "path = \"/Volumes/workspace/ecommerce/ecommerce_data/2019-Oct.csv\"\n",
    "\n",
    "# Read CSV\n",
    "df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "\n",
    "# Preview the data\n",
    "df.show(5)\n",
    "\n",
    "#check schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84b3131c-48fc-4c39-ac92-d56e0b03881c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta table 'ecommerce_oct2019' created successfully!\n"
     ]
    }
   ],
   "source": [
    "#Step 2 – Save DataFrame as Delta Table (managed table)\n",
    "\n",
    "# Databricks will manage the storage location automatically\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"ecommerce_oct2019\")\n",
    "print(\"Delta table 'ecommerce_oct2019' created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0349cc8-a42c-4330-8242-615e988a9527",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop table if exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS ecommerce_oct2019_sql\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4ecb2b5-668f-4b10-8ddf-e6bdd66e138d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta table 'ecommerce_oct2019_sql' created via SQL!\n"
     ]
    }
   ],
   "source": [
    "#Step 3 – Create a Delta table using SQL \n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE ecommerce_oct2019_sql\n",
    "    USING DELTA\n",
    "    AS SELECT * FROM ecommerce_oct2019   \n",
    "    \"\"\")\n",
    "print(\"Delta table 'ecommerce_oct2019_sql' created via SQL!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d106832-99e4-40f0-b34d-fcf743550117",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+\n|database|           tableName|isTemporary|\n+--------+--------------------+-----------+\n| default|   ecommerce_oct2019|      false|\n| default|ecommerce_oct2019...|      false|\n+--------+--------------------+-----------+\n\n+-------------------+----------+----------+-------------------+--------------------+-------+------+---------+--------------------+\n|         event_time|event_type|product_id|        category_id|       category_code|  brand| price|  user_id|        user_session|\n+-------------------+----------+----------+-------------------+--------------------+-------+------+---------+--------------------+\n|2019-10-23 09:44:06|      view|   2800638|2053013563835941749|appliances.kitche...|   NULL|194.32|523560041|f2da82d4-3aca-4eb...|\n|2019-10-23 09:44:06|      view|   1306746|2053013558920217191|  computers.notebook| lenovo|873.61|532561846|d2a62c6a-8529-469...|\n|2019-10-23 09:44:06|      view|   4700549|2053013560899928785|auto.accessories....|neoline|143.89|550105456|d4d16d85-4c7a-48e...|\n|2019-10-23 09:44:06|      view|  52100001|2137704926053138958|                NULL|   nike| 88.78|516571149|7fd09910-745e-440...|\n|2019-10-23 09:44:06|      view|   8801100|2053013555573162395|electronics.telep...|  nokia| 94.67|516822110|61d49429-0f6a-435...|\n+-------------------+----------+----------+-------------------+--------------------+-------+------+---------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Step 4: How to verify the table really exists:\n",
    "\n",
    "# List tables\n",
    "spark.sql(\"SHOW TABLES\").show()\n",
    "\n",
    "#Preview the data\n",
    "spark.sql(\"SELECT * FROM ecommerce_oct2019_sql LIMIT 5\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e204a3a-6cea-4267-bfb4-dc5434ffbd3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+-----------+--------+--------------------+-------------------+----------------+-----------------+--------+-----------+--------------------+----------------+----------------+--------------------+--------------------+-------------+\n|format|                  id|                name|description|location|           createdAt|       lastModified|partitionColumns|clusteringColumns|numFiles|sizeInBytes|          properties|minReaderVersion|minWriterVersion|       tableFeatures|          statistics|clusterByAuto|\n+------+--------------------+--------------------+-----------+--------+--------------------+-------------------+----------------+-----------------+--------+-----------+--------------------+----------------+----------------+--------------------+--------------------+-------------+\n| delta|7a471df9-654b-4df...|workspace.default...|       NULL|        |2026-01-12 04:13:...|2026-01-12 04:13:21|              []|               []|      12| 1471926065|{delta.enableDele...|               3|               7|[appendOnly, dele...|{numRowsDeletedBy...|        false|\n+------+--------------------+--------------------+-----------+--------+--------------------+-------------------+----------------+-----------------+--------+-----------+--------------------+----------------+----------------+--------------------+--------------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Check Delta details\n",
    "spark.sql(\"DESCRIBE DETAIL ecommerce_oct2019_sql\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88247b59-e7ad-4767-ae54-7b3bc3ac9fc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Schema Enforcement Test\n",
    "\n",
    "#Step 5: Create a DataFrame with WRONG schema\n",
    "from pyspark.sql import Row\n",
    "\n",
    "## Completely different schema\n",
    "wrong_schema_df = spark.createDataFrame([Row(id=1, name = \"Vid\", value=100)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c4c3bac-b2f3-49be-9634-df7e342789a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema enforcement triggered!\n[_LEGACY_ERROR_TEMP_DELTA_0007] A schema mismatch detected when writing to the Delta table (Table ID: 7a471df9-654b-4df5-971b-d94b8ed284b7).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- event_time: timestamp (nullable = true)\n-- event_type: string (nullable = true)\n-- product_id: integer (nullable = true)\n-- category_id: long (nullable = true)\n-- category_code: string (nullable = true)\n-- brand: string (nullable = true)\n-- price: double (nullable = true)\n-- user_id: integer (nullable = true)\n-- user_session: string (nullable = true)\n\n\nData schema:\nroot\n-- id: long (nullable = true)\n-- name: string (nullable = true)\n-- value: long (nullable = true)\n\n         \nTable ACLs are enabled in this cluster, so automatic schema migration is not allowed. Please use the ALTER TABLE command for changing the schema.         \n\nJVM stacktrace:\ncom.databricks.sql.transaction.tahoe.DeltaAnalysisException\n\tat com.databricks.sql.transaction.tahoe.MetadataMismatchErrorBuilder.finalizeAndThrow(DeltaErrors.scala:4310)\n\tat com.databricks.sql.transaction.tahoe.schema.ImplicitMetadataOperation.updateMetadata(ImplicitMetadataOperation.scala:231)\n\tat com.databricks.sql.transaction.tahoe.schema.ImplicitMetadataOperation.updateMetadata$(ImplicitMetadataOperation.scala:92)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.updateMetadata(WriteIntoDeltaEdge.scala:120)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.writeAndReturnCommitDataAndMaterializationPlans(WriteIntoDeltaEdge.scala:411)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.writeAndReturnCommitData(WriteIntoDeltaEdge.scala:256)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.$anonfun$run$2(WriteIntoDeltaEdge.scala:182)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:325)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.$anonfun$run$1(WriteIntoDeltaEdge.scala:169)\n\tat com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2642)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)\n\tat com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:235)\n\tat com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2642)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.run(WriteIntoDeltaEdge.scala:168)\n\tat com.databricks.sql.transaction.tahoe.catalog.WriteIntoDeltaBuilder$$anon$3$$anon$4.insert(DeltaTableV2.scala:1071)\n\tat org.apache.spark.sql.execution.datasources.v2.SupportsV1Write.writeWithV1(V1FallbackWriters.scala:136)\n\tat org.apache.spark.sql.execution.datasources.v2.SupportsV1Write.writeWithV1$(V1FallbackWriters.scala:117)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExecV1.writeWithV1(V1FallbackWriters.scala:35)\n\tat org.apache.spark.sql.execution.datasources.v2.V1FallbackWriters.run(V1FallbackWriters.scala:83)\n\tat org.apache.spark.sql.execution.datasources.v2.V1FallbackWriters.run$(V1FallbackWriters.scala:81)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExecV1.run(V1FallbackWriters.scala:35)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:596)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:596)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:595)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$18(SQLExecution.scala:600)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$16(SQLExecution.scala:513)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:932)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$15(SQLExecution.scala:434)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$14(SQLExecution.scala:434)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:967)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:433)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:255)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:885)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:591)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:587)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:528)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:585)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:702)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:694)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:543)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:543)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:519)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:694)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:694)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:484)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:489)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:730)\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:854)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveAsTable(DataFrameWriter.scala:702)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveAsTable(DataFrameWriter.scala:609)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:4092)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3450)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:281)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)\n"
     ]
    }
   ],
   "source": [
    "#Step 6: Try appending it to the Delta table\n",
    "try:\n",
    "    wrong_schema_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"ecommerce_oct2019_sql\")\n",
    "except Exception as e:\n",
    "    print(\"Schema enforcement triggered!\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a24c06d-e55f-479e-9c8a-ebec3a7f7e1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Day 5 TASK continues from Day 4 TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8759e304-a56d-46a4-a261-5cdc65b7959b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "STEP 7: Incremental MERGE (UPSERT)"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_updated_rows: bigint, num_deleted_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#STEP 7: Incremental MERGE (UPSERT)\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "#Get Delta table\n",
    "delta_table = DeltaTable.forName(spark, \"ecommerce_oct2019_sql\")\n",
    "\n",
    "#Simulate incremental data (price update + new rows)\n",
    "updates_df = df.limit(10)\\\n",
    "    .withColumn(\"price\", F.col(\"price\") + 100)\n",
    "\n",
    "#Merge\n",
    "delta_table.alias(\"t\").merge(\n",
    "    updates_df.alias(\"s\"),\n",
    "    \"t.event_time = s.event_time AND t.user_id = s.user_id\"\n",
    ").whenMatchedUpdateAll() \\\n",
    ".whenNotMatchedInsertAll() \\\n",
    ".execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "234cb49c-2e5c-4323-aa42-8e610678329b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 12"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+--------------+-------------------+----------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+------------------------+-----------+-----------------+-------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+--------------------------------------------------+\n|version|timestamp              |userId        |userName           |operation             |operationParameters                                                                                                                                                                                                                                                         |job |notebook          |clusterId               |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |userMetadata|engineInfo                                        |\n+-------+-----------------------+--------------+-------------------+----------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+------------------------+-----------+-----------------+-------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+--------------------------------------------------+\n|4      |2026-01-13 00:59:34.001|76030603077274|vidyarasu@gmail.com|VACUUM END            |{status -> COMPLETED}                                                                                                                                                                                                                                                       |NULL|{3472498303632321}|0113-003308-e57llh7-v2n |3          |SnapshotIsolation|true         |{numDeletedFiles -> 0, numVacuumedDirectories -> 1}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |NULL        |Databricks-Runtime/17.3.x-aarch64-photon-scala2.13|\n|3      |2026-01-13 00:59:34    |76030603077274|vidyarasu@gmail.com|VACUUM START          |{retentionCheckEnabled -> true, defaultRetentionMillis -> 604800000, specifiedRetentionMillis -> 604800000}                                                                                                                                                                 |NULL|{3472498303632321}|0113-003308-e57llh7-v2n |2          |SnapshotIsolation|true         |{numFilesToDelete -> 0, sizeOfDataToDelete -> 0}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |NULL        |Databricks-Runtime/17.3.x-aarch64-photon-scala2.13|\n|2      |2026-01-13 00:40:59    |76030603077274|vidyarasu@gmail.com|OPTIMIZE              |{clusterBy -> [], zOrderBy -> [\"event_type\",\"user_id\"], batchId -> 0, predicate -> [], auto -> false}                                                                                                                                                                       |NULL|{3472498303632321}|0113-003308-e57llh7-v2n |1          |SnapshotIsolation|false        |{numRemovedFiles -> 13, numRemovedBytes -> 1471929538, p25FileSize -> 40329905, numDeletionVectorsRemoved -> 1, minFileSize -> 28893845, p75FileSize -> 49562559, p50FileSize -> 44191695, numAddedBytes -> 1158113327, numAddedFiles -> 26, maxFileSize -> 56077365}                                                                                                                                                                                                                                                                                                                                                                                                                     |NULL        |Databricks-Runtime/17.3.x-aarch64-photon-scala2.13|\n|1      |2026-01-12 22:01:31    |76030603077274|vidyarasu@gmail.com|MERGE                 |{predicate -> [\"((event_time#13240 = event_time#13181) AND (user_id#13247 = user_id#13188))\"], clusterBy -> [], matchedPredicates -> [{\"actionType\":\"update\"}], statsOnLoad -> false, notMatchedBySourcePredicates -> [], notMatchedPredicates -> [{\"actionType\":\"insert\"}]}|NULL|{3472498303632321}|0112-215546-1r8n7lgi-v2n|0          |WriteSerializable|false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetBytesRemoved -> 0, numTargetDeletionVectorsAdded -> 1, numTargetRowsMatchedUpdated -> 10, numTargetRowsMatchedDeleted -> 0, numTargetRowsUpdated -> 10, numTargetChangeFilesAdded -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 4039, numTargetFilesAdded -> 1, numTargetBytesAdded -> 3473, executionTimeMs -> 9377, materializeSourceTimeMs -> 1481, numTargetRowsInserted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 3684, numOutputRows -> 10, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numSourceRows -> 10, numTargetFilesRemoved -> 0}|NULL        |Databricks-Runtime/17.3.x-aarch64-photon-scala2.13|\n|0      |2026-01-12 04:13:21    |76030603077274|vidyarasu@gmail.com|CREATE TABLE AS SELECT|{partitionBy -> [], clusterBy -> [], description -> NULL, isManaged -> true, properties -> {\"delta.enableDeletionVectors\":\"true\"}, statsOnLoad -> true}                                                                                                                     |NULL|{3472498303632321}|0112-024414-5evbrvb0-v2n|NULL       |WriteSerializable|true         |{numFiles -> 12, numOutputRows -> 42448764, numOutputBytes -> 1471926065}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |NULL        |Databricks-Runtime/17.3.x-aarch64-photon-scala2.13|\n+-------+-----------------------+--------------+-------------------+----------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+------------------------+-----------+-----------------+-------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+--------------------------------------------------+\n\n+-------------------+----------+----------+-------------------+--------------------+-------+------+---------+--------------------+\n|         event_time|event_type|product_id|        category_id|       category_code|  brand| price|  user_id|        user_session|\n+-------------------+----------+----------+-------------------+--------------------+-------+------+---------+--------------------+\n|2019-10-23 09:44:06|      view|   2800638|2053013563835941749|appliances.kitche...|   NULL|194.32|523560041|f2da82d4-3aca-4eb...|\n|2019-10-23 09:44:06|      view|   1306746|2053013558920217191|  computers.notebook| lenovo|873.61|532561846|d2a62c6a-8529-469...|\n|2019-10-23 09:44:06|      view|   4700549|2053013560899928785|auto.accessories....|neoline|143.89|550105456|d4d16d85-4c7a-48e...|\n|2019-10-23 09:44:06|      view|  52100001|2137704926053138958|                NULL|   nike| 88.78|516571149|7fd09910-745e-440...|\n|2019-10-23 09:44:06|      view|   8801100|2053013555573162395|electronics.telep...|  nokia| 94.67|516822110|61d49429-0f6a-435...|\n|2019-10-23 09:44:06|      view|   1004133|2053013555631882655|electronics.smart...| xiaomi|141.58|531517925|3e78d161-8a64-483...|\n|2019-10-23 09:44:06|      view|  19200109|2053013556202308035|construction.tool...|  stihl|604.91|557991360|12af05ac-14fe-48e...|\n|2019-10-23 09:44:06|      cart|  18500036|2053013552695869677|  electronics.tablet|  wacom|112.85|513934614|9cdc8f56-4c1f-4d0...|\n|2019-10-23 09:44:06|      view|  42200110|2095518917320508073|                NULL|   NULL| 13.59|540398673|90d17e49-f2ad-432...|\n|2019-10-23 09:44:06|      view|  15800070|2053013560144954031|                NULL|  huter| 70.76|543044394|eefdcf60-2b16-40e...|\n|2019-10-23 09:44:06|      view|  28721262|2116907519078040377|       apparel.shoes|  legre|118.15|514264137|984b820c-8c13-4b8...|\n|2019-10-23 09:44:06|      cart|   1004775|2053013555631882655|electronics.smart...| xiaomi|183.27|533153067|72249b5c-42c5-444...|\n|2019-10-23 09:44:06|      view|  20100092|2053013559473865347|                NULL|   nika| 68.57|559511385|4dc32b19-3191-4c6...|\n|2019-10-23 09:44:06|      view|   1005174|2053013555631882655|electronics.smart...|samsung|643.23|525907702|b3fefbe3-5312-4a3...|\n|2019-10-23 09:44:06|      view|   3700779|2053013565983425517|appliances.enviro...| xiaomi|297.29|517106943|e3983aa3-9e20-47a...|\n|2019-10-23 09:44:07|      view|   1004720|2053013555631882655|electronics.smart...| huawei|126.13|563271738|fd749ae6-b0cb-46f...|\n|2019-10-23 09:44:07|      view|  15100370|2053013557024391671|                NULL|   NULL|257.15|563271469|97a04e92-66db-4a6...|\n|2019-10-23 09:44:07|      view|   1004565|2053013555631882655|electronics.smart...| huawei|165.96|531522372|7470242e-0367-463...|\n|2019-10-23 09:44:07|      view|   1004849|2053013555631882655|electronics.smart...| huawei|952.03|525236708|11e03503-ad2e-415...|\n|2019-10-23 09:44:07|      view|   1480733|2053013561092866779|   computers.desktop| pulser|710.42|561043223|396536dd-f2cd-4bd...|\n+-------------------+----------+----------+-------------------+--------------------+-------+------+---------+--------------------+\nonly showing top 20 rows\n+-------------------+----------+----------+-------------------+--------------------+-------+------+---------+--------------------+\n|         event_time|event_type|product_id|        category_id|       category_code|  brand| price|  user_id|        user_session|\n+-------------------+----------+----------+-------------------+--------------------+-------+------+---------+--------------------+\n|2019-10-23 09:44:06|      view|   2800638|2053013563835941749|appliances.kitche...|   NULL|194.32|523560041|f2da82d4-3aca-4eb...|\n|2019-10-23 09:44:06|      view|   1306746|2053013558920217191|  computers.notebook| lenovo|873.61|532561846|d2a62c6a-8529-469...|\n|2019-10-23 09:44:06|      view|   4700549|2053013560899928785|auto.accessories....|neoline|143.89|550105456|d4d16d85-4c7a-48e...|\n|2019-10-23 09:44:06|      view|  52100001|2137704926053138958|                NULL|   nike| 88.78|516571149|7fd09910-745e-440...|\n|2019-10-23 09:44:06|      view|   8801100|2053013555573162395|electronics.telep...|  nokia| 94.67|516822110|61d49429-0f6a-435...|\n|2019-10-23 09:44:06|      view|   1004133|2053013555631882655|electronics.smart...| xiaomi|141.58|531517925|3e78d161-8a64-483...|\n|2019-10-23 09:44:06|      view|  19200109|2053013556202308035|construction.tool...|  stihl|604.91|557991360|12af05ac-14fe-48e...|\n|2019-10-23 09:44:06|      cart|  18500036|2053013552695869677|  electronics.tablet|  wacom|112.85|513934614|9cdc8f56-4c1f-4d0...|\n|2019-10-23 09:44:06|      view|  42200110|2095518917320508073|                NULL|   NULL| 13.59|540398673|90d17e49-f2ad-432...|\n|2019-10-23 09:44:06|      view|  15800070|2053013560144954031|                NULL|  huter| 70.76|543044394|eefdcf60-2b16-40e...|\n|2019-10-23 09:44:06|      view|  28721262|2116907519078040377|       apparel.shoes|  legre|118.15|514264137|984b820c-8c13-4b8...|\n|2019-10-23 09:44:06|      cart|   1004775|2053013555631882655|electronics.smart...| xiaomi|183.27|533153067|72249b5c-42c5-444...|\n|2019-10-23 09:44:06|      view|  20100092|2053013559473865347|                NULL|   nika| 68.57|559511385|4dc32b19-3191-4c6...|\n|2019-10-23 09:44:06|      view|   1005174|2053013555631882655|electronics.smart...|samsung|643.23|525907702|b3fefbe3-5312-4a3...|\n|2019-10-23 09:44:06|      view|   3700779|2053013565983425517|appliances.enviro...| xiaomi|297.29|517106943|e3983aa3-9e20-47a...|\n|2019-10-23 09:44:07|      view|   1004720|2053013555631882655|electronics.smart...| huawei|126.13|563271738|fd749ae6-b0cb-46f...|\n|2019-10-23 09:44:07|      view|  15100370|2053013557024391671|                NULL|   NULL|257.15|563271469|97a04e92-66db-4a6...|\n|2019-10-23 09:44:07|      view|   1004565|2053013555631882655|electronics.smart...| huawei|165.96|531522372|7470242e-0367-463...|\n|2019-10-23 09:44:07|      view|   1004849|2053013555631882655|electronics.smart...| huawei|952.03|525236708|11e03503-ad2e-415...|\n|2019-10-23 09:44:07|      view|   1480733|2053013561092866779|   computers.desktop| pulser|710.42|561043223|396536dd-f2cd-4bd...|\n+-------------------+----------+----------+-------------------+--------------------+-------+------+---------+--------------------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# STEP 8: Time Travel (read old version)\n",
    "#1. Check history\n",
    "spark.sql(\"DESCRIBE HISTORY ecommerce_oct2019_sql\").show(truncate = False)\n",
    "\n",
    "# 2. Read first version\n",
    "old_version = spark.read.format(\"delta\") \\\n",
    "    .option (\"versionAsOf\",0) \\\n",
    "    .table(\"ecommerce_oct2019_sql\") \n",
    "old_version.show(20)\n",
    "\n",
    "# 3. Read by timestamp (choose valid timestamp from history)\n",
    "yesterday = spark.read.format(\"delta\") \\\n",
    "    .option(\"timestampAsOf\", \"2026-01-12 04:13:21\") \\\n",
    "    .table(\"ecommerce_oct2019_sql\") #after the first UPSERT (MERGE) but before OPTIMIZE/VACUUM\n",
    "    \n",
    "yesterday.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b2a6dc0-5f21-41f2-b452-b7771a930eba",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 13"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n|path|             metrics|\n+----+--------------------+\n|    |{0, 0, {NULL, NUL...|\n+----+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#STEP 9: OPTIMIZE & ZORDER\n",
    "spark.sql(\"\"\"OPTIMIZE ecommerce_oct2019_sql \n",
    "          ZORDER BY (event_type, user_id)\n",
    "          \"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49dcd8c1-34cd-4de4-a586-02b7cffbf220",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n|path|\n+----+\n|    |\n+----+\n\n"
     ]
    }
   ],
   "source": [
    "#STEP 10: Clean up old files (safe retention)\n",
    "spark.sql(\"\"\"\n",
    "          VACUUM ecommerce_oct2019_sql RETAIN 168 HOURS\n",
    "          \"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Day_05 Task_Vidhya Rasu",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}